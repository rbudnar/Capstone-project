{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "# from keras.applications.densenet import DenseNet121\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Add, Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import timeit\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "from training_mode import TrainingMode\n",
    "\n",
    "from main_v2 import (generate_dataframe_from_csv_horizontal, generate_dataframe_from_csv_vertical, \n",
    "                         get_model_inputs, create_multi_generator, build_model, ConvBlock)\n",
    "# https://github.com/keras-team/keras/issues/4161#issuecomment-366031228\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from train import TrainingRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6145 2048 769 256\n"
     ]
    }
   ],
   "source": [
    "# df = generate_dataframe_from_csv_vertical(\"train.csv\", inputs=1)\n",
    "# df = generate_dataframe_from_csv_horizontal(\"train.csv\")\n",
    "df = pd.read_csv(\"./train_controls_root.csv\",  dtype={'sirna': object})\n",
    "val_split = 0.25\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "train_size = int(len(df)*(1-val_split))\n",
    "valid_size = int(len(df)*(val_split))\n",
    "steps = ceil(train_size/batch_size)\n",
    "steps_valid = ceil(valid_size/batch_size)\n",
    "print(train_size, valid_size, steps, steps_valid)\n",
    "# y = df[\"sirna\"]\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#runner = TrainingRunner(filename=\"train_root.csv\", use_multi=True, epochs=epochs, batch_size=batch_size, val_split=val_split, use_wandb=False)\n",
    "# runner = TrainingRunner(batch_size=16, filename=\"train_root.csv\", train_mode=TrainingMode.CELL_MULTI)\n",
    "# runner = TrainingRunner(batch_size=16, train_mode=TrainingMode.SINGLE)\n",
    "runner = TrainingRunner(filename=\"train_controls_root.csv\", batch_size=16, controls_only=True, train_mode=TrainingMode.MULTI)\n",
    "runner.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# runner.train_generator.__getitem__(1)\n",
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.model.save(\"saved_models\\cell_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54773 validated image filenames belonging to 1108 classes.\n",
      "Found 18257 validated image filenames belonging to 1108 classes.\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"testing-ml\")\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
    "                                   verbose=1, mode='auto', min_delta=0.0001)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=15)\n",
    "\n",
    "csv_logger = CSVLogger(filename='./training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "(images, trainY) = get_model_inputs(df)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=val_split)\n",
    "\n",
    "# train_generator = create_multi_generator(df, train_datagen, \"training\") \n",
    "# valid_generator = create_multi_generator(df, train_datagen, \"validation\") \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"img_path\",\n",
    "        y_col=\"sirna\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"img_path\",\n",
    "        y_col=\"sirna\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        subset=\"validation\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 128)     512       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              129000    \n",
      "_________________________________________________________________\n",
      "bn_fc_1 (BatchNormalization) (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1108)              1109108   \n",
      "=================================================================\n",
      "Total params: 1,321,596\n",
      "Trainable params: 1,319,212\n",
      "Non-trainable params: 2,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1712/1712 [==============================] - 678s 396ms/step - loss: 7.5768 - acc: 0.0012 - val_loss: 16.4520 - val_acc: 8.2160e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 16.45205, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/20\n",
      "  41/1712 [..............................] - ETA: 9:26 - loss: 7.3360 - acc: 0.0023"
     ]
    }
   ],
   "source": [
    "#https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "# WORKERS = 2\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=steps,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=steps_valid,\n",
    "#                     workers=WORKERS, \n",
    "#                     use_multiprocessing=True,\n",
    "                    epochs=epochs, callbacks=[checkpointer,reduceLROnPlat, early, csv_logger, tensorboard_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb run python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
