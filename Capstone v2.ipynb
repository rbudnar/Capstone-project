{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Add, Concatenate, BatchNormalization\n",
    "# # from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Add, Concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import timeit\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from math import ceil\n",
    "from main_v2 import (generate_dataframe_from_csv_horizontal, generate_dataframe_from_csv_vertical, \n",
    "                         get_model_inputs, create_multi_generator, build_model, ConvBlock)\n",
    "# https://github.com/keras-team/keras/issues/4161#issuecomment-366031228\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from train import TrainingRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54772 18257 1712 571\n"
     ]
    }
   ],
   "source": [
    "# df = generate_dataframe_from_csv_vertical(\"train.csv\", inputs=1)\n",
    "# df = generate_dataframe_from_csv_horizontal(\"train.csv\")\n",
    "df = pd.read_csv(\"./train_1.csv\",  dtype={'sirna': object})\n",
    "val_split = 0.25\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "train_size = int(len(df)*(1-val_split))\n",
    "valid_size = int(len(df)*(val_split))\n",
    "steps = ceil(train_size/batch_size)\n",
    "steps_valid = ceil(valid_size/batch_size)\n",
    "print(train_size, valid_size, steps, steps_valid)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = TrainingRunner(filename=\"train_1.csv\", epochs=epochs, batch_size=batch_size, val_split=val_split, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54773 validated image filenames belonging to 1108 classes.\n",
      "Found 18257 validated image filenames belonging to 1108 classes.\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"testing-ml\")\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
    "                                   verbose=1, mode='auto', min_delta=0.0001)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=15)\n",
    "\n",
    "csv_logger = CSVLogger(filename='./training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "(images, trainY) = get_model_inputs(df)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=val_split)\n",
    "\n",
    "# train_generator = create_multi_generator(df, train_datagen, \"training\") \n",
    "# valid_generator = create_multi_generator(df, train_datagen, \"validation\") \n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"img_path\",\n",
    "        y_col=\"sirna\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        subset=\"training\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"img_path\",\n",
    "        y_col=\"sirna\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        subset=\"validation\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              513000    \n",
      "_________________________________________________________________\n",
      "bn_fc_1 (BatchNormalization) (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1108)              1109108   \n",
      "=================================================================\n",
      "Total params: 5,583,036\n",
      "Trainable params: 5,577,964\n",
      "Non-trainable params: 5,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1712/1712 [==============================] - 274s 160ms/step - loss: 8.2231 - acc: 0.0028 - val_loss: 9.7716 - val_acc: 0.0019\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.77161, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/20\n",
      "1712/1712 [==============================] - 264s 154ms/step - loss: 7.0247 - acc: 0.0080 - val_loss: 9.2405 - val_acc: 0.0046\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.77161 to 9.24046, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/20\n",
      "1712/1712 [==============================] - 262s 153ms/step - loss: 6.6799 - acc: 0.0151 - val_loss: 10.0986 - val_acc: 0.0035\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.24046\n",
      "Epoch 4/20\n",
      "1712/1712 [==============================] - 261s 153ms/step - loss: 6.5309 - acc: 0.0238 - val_loss: 10.1187 - val_acc: 0.0052\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.24046\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/20\n",
      "1712/1712 [==============================] - 262s 153ms/step - loss: 5.9469 - acc: 0.0557 - val_loss: 9.2256 - val_acc: 0.0084\n",
      "\n",
      "Epoch 00005: val_loss improved from 9.24046 to 9.22556, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/20\n",
      "1712/1712 [==============================] - 264s 154ms/step - loss: 5.5840 - acc: 0.0843 - val_loss: 11.9087 - val_acc: 0.0078\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.22556\n",
      "Epoch 7/20\n",
      "1712/1712 [==============================] - 266s 155ms/step - loss: 5.2754 - acc: 0.1152 - val_loss: 11.8382 - val_acc: 0.0188\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.22556\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "1712/1712 [==============================] - 266s 156ms/step - loss: 4.5986 - acc: 0.2093 - val_loss: 11.6636 - val_acc: 0.0221\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.22556\n",
      "Epoch 9/20\n",
      "1712/1712 [==============================] - 267s 156ms/step - loss: 4.1507 - acc: 0.2763 - val_loss: 11.7251 - val_acc: 0.0220\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.22556\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/20\n",
      "1712/1712 [==============================] - 266s 155ms/step - loss: 3.5439 - acc: 0.3994 - val_loss: 12.0089 - val_acc: 0.0220\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.22556\n",
      "Epoch 11/20\n",
      "1712/1712 [==============================] - 267s 156ms/step - loss: 3.2264 - acc: 0.4602 - val_loss: 12.3828 - val_acc: 0.0205\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 9.22556\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 12/20\n",
      " 114/1712 [>.............................] - ETA: 3:15 - loss: 2.8376 - acc: 0.5554"
     ]
    }
   ],
   "source": [
    "#https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "# WORKERS = 2\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=steps,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=steps_valid,\n",
    "#                     workers=WORKERS, \n",
    "#                     use_multiprocessing=True,\n",
    "                    epochs=epochs, callbacks=[checkpointer,reduceLROnPlat, early, csv_logger, tensorboard_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb run python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard --logdir=logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
